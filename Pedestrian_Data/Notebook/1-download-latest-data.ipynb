{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit ('pedvenv': venv)"
  },
  "interpreter": {
   "hash": "021c0ff7babdd78dcf1d264e8c76199d3d723a312b5a0fe9bc60f5394c2d1ecd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Download the latest data\n",
    "There are up to four calls that need to be made to download or update the pedestrian safety data.\n",
    "- Mapillary's road data\n",
    "- Mapillary's signage data\n",
    "- LAPD's collision data\n",
    "- The neighborhood council boundaries shapefile"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import pandas as pd\n",
    "import requests\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Load the mapillary client module\n",
    "try:\n",
    "    os.chdir(os.path.dirname(os.path.realpath(__file__)))\n",
    "except:\n",
    "    pass\n",
    "mapillarywrapperdir = '../../311-data/mapillarywrapper/'\n",
    "sys.path.append(mapillarywrapperdir)\n",
    "from mapillarywrapper.client import client\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(mapillarywrapperdir + 'mapillary.cfg')\n",
    "CLIENT_ID = config['CLIENT-ID']['CLIENT_ID']\n",
    "mapillaryclient = client.MapClient(CLIENT_ID)\n",
    "\n"
   ]
  },
  {
   "source": [
    "Set the save locations of the raw data csv files."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdatadir = '../Data/1-raw-data/'\n",
    "rawroaddatacsv = rawdatadir + 'raw-road-data.csv'\n",
    "rawsigndatacsv = rawdatadir + 'raw-sign-data.csv'\n",
    "rawcollisiondatacsv = rawdatadir + 'raw-collision-data.csv'\n",
    "neighborhoodcouncilshapesgeojson = rawdatadir + 'neighborhoodcouncils.geojson'"
   ]
  },
  {
   "source": [
    "## Bounding box\n",
    "\n",
    "This is the latitude/longitude box that encompasses the City of LA according to https://boundingbox.klokantech.com"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laLowerLeft = [33.703622, -118.668187]\n",
    "laUpperRight = [34.337306, -118.155295]"
   ]
  },
  {
   "source": [
    "## Merging method\n",
    "This will combine dataframes of the same dataset downloaded at different times or with different filters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinedataframes(\n",
    "    leftdf: pd.DataFrame, rightdf: pd.DataFrame, keycolumn: str\n",
    "):\n",
    "  \"\"\"Combine dataframes of the same dataset downloaded at different times or with different filters.\"\"\"\n",
    "\n",
    "  # Use the keycolumn as the index with a different name, to differentiate them.\n",
    "  leftreindexed = leftdf.set_index(keycolumn, drop=False)\n",
    "  leftreindexed.index.name = 'indexkeys'\n",
    "  rightreindexed = rightdf.set_index(keycolumn, drop=False)\n",
    "  rightreindexed.index.name = 'indexkeys'\n",
    "\n",
    "  # Merge the keys, creating empty key-only rows for all new data\n",
    "  allrows = leftreindexed.merge(\n",
    "      rightreindexed[keycolumn], left_index=True, right_index=True, how='outer'\n",
    "  ).drop(columns=[keycolumn + '_x', keycolumn + '_y'])\n",
    "\n",
    "  # Update all rows with the newest data\n",
    "  allrows.update(rightreindexed)\n",
    "  \n",
    "  # copy the keys from the index back into a regular column\n",
    "  allrows[keycolumn] = allrows.index\n",
    "  allrows.reset_index(drop=True, inplace=True)\n",
    "  return allrows\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "### ISO Date methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def fromapidateformat(isostring: str) -> datetime.datetime:\n",
    "  return datetime.datetime.fromisoformat(isostring.strip('Z'))\n",
    "\n",
    "\n",
    "def toapidateformat(dt: datetime.datetime):\n",
    "  return dt.isoformat('T', 'milliseconds') + 'Z'"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Download data from mapillary\n",
    "If mapillary data is saved locally, download the latest. If it isn't, download everything."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for [mapillarylayer, csvlocation] in [\n",
    "  ['points', rawroaddatacsv], ['trafficsigns', rawsigndatacsv]\n",
    "]:\n",
    "  savefileexists = os.path.isfile(csvlocation)\n",
    "  if savefileexists:\n",
    "    previousdownload = pd.read_csv(csvlocation)\n",
    "    mostrecentdata = previousdownload['last_seen_at'].map(\n",
    "      fromapidateformat\n",
    "    ).max()\n",
    "    datacutoffdate = mostrecentdata - datetime.timedelta(days=7)\n",
    "\n",
    "    newmapillarydata = pd.DataFrame(\n",
    "      mapillaryclient.trafficinfo(\n",
    "        laLowerLeft,\n",
    "        laUpperRight,\n",
    "        perpage=1000,\n",
    "        layer=mapillarylayer,\n",
    "        startlastseenat=toapidateformat(datacutoffdate.to_pydatetime())\n",
    "      )\n",
    "    )\n",
    "\n",
    "    fulldataset = combinedataframes(previousdownload, newmapillarydata, 'key')\n",
    "\n",
    "  else:\n",
    "    fulldataset = pd.DataFrame(\n",
    "      mapillaryclient.trafficinfo(\n",
    "        laLowerLeft,\n",
    "        laUpperRight,\n",
    "        perpage=1000,\n",
    "        layer=mapillarylayer,\n",
    "      )\n",
    "    )\n",
    "\n",
    "  fulldataset.to_csv(csvlocation, index=False)\n",
    "  print(f'Mapillary {mapillarylayer} dataset saved to {csvlocation}')"
   ]
  },
  {
   "source": [
    "# Download LAPD Traffic collision data\n",
    "\n",
    "Using the city's [traffic collision dataset](https://data.lacity.org/Public-Safety/Traffic-Collision-Data-from-2010-to-Present/d5tf-ez2w). If the file doesn't exist, download everything. If it does exist, download everything added or updated since the last recorded change."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = \"https://data.lacity.org/resource/d5tf-ez2w.csv\"\n",
    "recordsperpage = 50000\n",
    "queryVisibleAndInvisibleFields = ':*,*'\n",
    "\n",
    "def getCollisionDataPageURL(pagenum: int, additionalParams: dict = {}):\n",
    "  req = requests.PreparedRequest()\n",
    "  req.prepare_url(\n",
    "    baseurl,\n",
    "    params={\n",
    "      '$limit': recordsperpage,\n",
    "      '$offset': recordsperpage * pagenum,\n",
    "      '$select': queryVisibleAndInvisibleFields,\n",
    "      '$order': ':id',\n",
    "      **additionalParams\n",
    "    }\n",
    "    # NOTE - data.lacity.org may start throttling after many requests. If that happens, we can signup for an app token.\n",
    "  )\n",
    "  return req.url\n",
    "\n",
    "\n",
    "def downloadLACollisions(additionalParams: dict = {}):\n",
    "  pagenum = 0\n",
    "  firstpageURL = getCollisionDataPageURL(pagenum, additionalParams)\n",
    "  collisiondatapages = [pd.read_csv(firstpageURL)]\n",
    "  while (len(collisiondatapages[-1]) == recordsperpage):\n",
    "    pagenum = pagenum + 1\n",
    "    nextpageURL = getCollisionDataPageURL(pagenum)\n",
    "    collisiondatapages.append(pd.read_csv(nextpageURL))\n",
    "\n",
    "  alldownloadedcollisiondata = pd.concat(collisiondatapages, ignore_index=True)\n",
    "  return alldownloadedcollisiondata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for last saved datarow, if it exists.\n",
    "if os.path.isfile(rawcollisiondatacsv):\n",
    "    previousdownload = pd.read_csv(rawcollisiondatacsv)\n",
    "    lastchange = pd.concat([previousdownload[':created_at'], previousdownload[':updated_at']], ignore_index=True).map(fromapidateformat).max().to_pydatetime()\n",
    "\n",
    "    newrows = downloadLACollisions({\"$where\": f\":created_at > '{toapidateformat(lastchange)}'\"})\n",
    "    print(f\"New rows downloaded: {len(newrows)}\")\n",
    "    newrowsadded = combinedataframes(previousdownload, newrows, \":id\")\n",
    "\n",
    "    updatedrows = downloadLACollisions({\"$where\": f\":updated_at > '{toapidateformat(lastchange)}'\"})\n",
    "    print(f\"Updated rows downloaded: {len(updatedrows)}\")\n",
    "    fullcollisiondataset = combinedataframes(newrowsadded, updatedrows, \":id\")\n",
    "else:\n",
    "    fullcollisiondataset = downloadLACollisions()\n",
    "    print(f\"Downloaded {len(fullcollisiondataset)} rows.\")\n",
    "\n",
    "fullcollisiondataset.to_csv(rawcollisiondatacsv, index=False)\n",
    "print(f\"Los Angeles collisions dataset saved to {rawcollisiondatacsv}\")\n"
   ]
  },
  {
   "source": [
    "# Download neighborhood council shapefile\n",
    "\n",
    "Website: https://geohub.lacity.org/datasets/neighborhood-councils-certified/explore\n",
    "\n",
    "API: https://opendata.arcgis.com/datasets/9c8639737e3a457a8c0f6a93f9c36974_18.geojson"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "ncshapefileresponse = requests.get('https://opendata.arcgis.com/datasets/9c8639737e3a457a8c0f6a93f9c36974_18.geojson')\n",
    "with open(neighborhoodcouncilshapesgeojson, \"wb\") as geojsonfile:\n",
    "    geojsonfile.write(ncshapefileresponse.content)\n",
    "print(f\"Saved shapefile to {neighborhoodcouncilshapesgeojson}\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}